{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df00974",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 1000;\n",
    "    color:white;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    "img {\n",
    "    display: block;\n",
    "    margin: 0 auto;\n",
    "    width: 40em;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2>CHAPTER 2: Tokens and Embeddings</h2>\n",
    "<p><b>Tokens and Embeddings</b> are two of the central concepts of using large language models (LLMs). In this chapter, we look more closely at what tokens are and the tokenization methods used to power LLMs. We will then dive into the famous <b>word2vec embeddings</b> method tha preceded modern-day LLms and see how it's extending the concept of token embeddings to build commercial recommendation systems that power a lot of the apps you use.</p>\n",
    "<img src=\"../image/tokens_and_embeddings.jpg\">\n",
    "<p>A model does not produce its output response all at once; it actually generates one token at a time. But tokens aren't only the output of a model, they're also the way in which the model sees its inputs. A text prompt sent to the model is first broken down into tokens. Before the prompt is presented to the language model, however, it first has to go through a <b>tokenizer</b> that breaks it into pieces. Here's an example that show the tokenizer of <b>GPT-3.5 and GPT-4</b>.</p>\n",
    "<img src=\"../image/tokenizer_example.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de80b0",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: 1000\">Run an example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6779bcf-ec2d-44f7-b9d9-2e06abc99dcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:41:05.070624Z",
     "iopub.status.busy": "2025-08-18T13:41:05.070346Z",
     "iopub.status.idle": "2025-08-18T13:42:21.585284Z",
     "shell.execute_reply": "2025-08-18T13:42:21.584329Z",
     "shell.execute_reply.started": "2025-08-18T13:41:05.070563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers numpy pandas \n",
    "!pip install -q torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a2b62-3220-4c0d-a19b-52a3b7ed14ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:42:47.203674Z",
     "iopub.status.busy": "2025-08-18T13:42:47.203384Z",
     "iopub.status.idle": "2025-08-18T13:44:20.952021Z",
     "shell.execute_reply": "2025-08-18T13:44:20.950877Z",
     "shell.execute_reply.started": "2025-08-18T13:42:47.203644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f09c093-ebcd-4a5e-a127-3a00ddc1c658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:52:57.388844Z",
     "iopub.status.busy": "2025-08-18T13:52:57.387944Z",
     "iopub.status.idle": "2025-08-18T13:55:37.778984Z",
     "shell.execute_reply": "2025-08-18T13:55:37.778294Z",
     "shell.execute_reply.started": "2025-08-18T13:52:57.388809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWrite an example of Part 1 IELTS writing band 9.0. <assistant>\\nIn the wake of the global pandemic, the importance of digital literacy has become more pronounced than ever. As we navigate through an increasingly digital world, the ability to effectively use technology is no longer a luxury but a necessity. This essay will explore the significance of digital literacy in today's society, its impact on various aspects of life, and the need for its integration into educational curricula.\\n\\nDigital literacy refers to the ability to use digital technology, communication tools, and networks to access, manage, integrate, evaluate, and create information. It encompasses a wide range of skills, including basic computer skills, internet navigation, online communication, and the ability to critically evaluate digital content. In a world where information is readily available at our fingertips, the ability to discern credible sources from unreliable ones is crucial.\\n\\nThe importance of digital literacy can be seen in various aspects of life. In the workplace, for instance, employees are expected to use digital tools to communicate, collaborate, and complete tasks efficiently. Those who lack digital literacy skills may find themselves at a disadvantage, struggling to keep up with their more digitally savvy colleagues\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write an example of Part 1 IELTS writing band 9.0. <assistant>\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt,\n",
    "                     return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "generated_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "text = tokenizer.decode(generated_output[0])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50dcec57-2e5e-4044-94c8-317bb0390aed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:08:54.357156Z",
     "iopub.status.busy": "2025-08-18T14:08:54.356528Z",
     "iopub.status.idle": "2025-08-18T14:08:54.370931Z",
     "shell.execute_reply": "2025-08-18T14:08:54.370053Z",
     "shell.execute_reply.started": "2025-08-18T14:08:54.357133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[29871,    13,  6113,   385,  1342,   310,  3455, 29871, 29896,  7159,\n",
       "          5850, 29903,  5007,  3719, 29871, 29929, 29889, 29900, 29889,   529,\n",
       "           465, 22137, 29958,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060036c-81a1-4a69-ad38-90844a1e7010",
   "metadata": {},
   "source": [
    "To view each token in the **input_ids**, we should read each id and then use the tokeniser to decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f29170-e224-419e-8377-76cd3d85c71c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:12:50.324325Z",
     "iopub.status.busy": "2025-08-18T14:12:50.324009Z",
     "iopub.status.idle": "2025-08-18T14:12:50.331485Z",
     "shell.execute_reply": "2025-08-18T14:12:50.330671Z",
     "shell.execute_reply.started": "2025-08-18T14:12:50.324304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Write\n",
      "an\n",
      "example\n",
      "of\n",
      "Part\n",
      "\n",
      "1\n",
      "IE\n",
      "LT\n",
      "S\n",
      "writing\n",
      "band\n",
      "\n",
      "9\n",
      ".\n",
      "0\n",
      ".\n",
      "<\n",
      "ass\n",
      "istant\n",
      ">\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id in input_ids[0]:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33569716-9eed-4948-8f65-8612a991e281",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 1000;\n",
    "    color:white;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    "img {\n",
    "    display: block;\n",
    "    margin: 0 auto;\n",
    "    width: 40em;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p>\n",
    "    Thanks to the example, we can know how the tonkeniser broke down our input prompt. Notice the following:\n",
    "    <ul>\n",
    "        <li>In some prompts, the first token <b>&lt;s&gt;</b>, a special token indacating the beginning of the text.</li>\n",
    "        <li>Some tokens are complete words.</li>\n",
    "        <li>Some tokens are parts of words</li>\n",
    "        <li>Punctuation characters are their token.</li>\n",
    "    </ul>\n",
    "    <p>One thing you should notice is that the space character does not have its own token. Instead, partial tokens (like \"IE\") have a special hidden character at their beginning that indicates that they're connected with the token that precedes them in the text. Tokens without that special character are assumed to have a space before them.</p>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7611b7c-b504-4799-a6fe-312d93949188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:22:20.482085Z",
     "iopub.status.busy": "2025-08-18T14:22:20.481332Z",
     "iopub.status.idle": "2025-08-18T14:22:20.492758Z",
     "shell.execute_reply": "2025-08-18T14:22:20.491988Z",
     "shell.execute_reply.started": "2025-08-18T14:22:20.482061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29871,    13,  6113,   385,  1342,   310,  3455, 29871, 29896,  7159,\n",
       "         5850, 29903,  5007,  3719, 29871, 29929, 29889, 29900, 29889,   529,\n",
       "          465, 22137, 29958,    13,   797,   278,   281,  1296,   310,   278,\n",
       "         5534,  7243, 24552, 29892,   278, 13500,   310, 13436,  4631,  4135,\n",
       "          756,  4953,   901, 11504, 20979,  1135,  3926, 29889,  1094,   591,\n",
       "        23624,  1549,   385, 10231,   368, 13436,  3186, 29892,   278, 11509,\n",
       "          304, 17583,   671, 15483,   338,   694,  5520,   263, 21684,  2857,\n",
       "          541,   263, 24316, 29889,   910,  3686,   388,   674, 26987,   278,\n",
       "        26002,   310, 13436,  4631,  4135,   297,  9826, 29915, 29879, 12459,\n",
       "        29892,   967, 10879,   373,  5164, 21420,   310,  2834, 29892,   322,\n",
       "          278,   817,   363,   967, 13465,   964, 28976, 16256,   293,  2497,\n",
       "        29889,    13,    13, 27103,  4631,  4135, 14637,   304,   278, 11509,\n",
       "          304,   671, 13436, 15483, 29892, 12084,  8492, 29892,   322, 14379,\n",
       "          304,  2130, 29892, 10933, 29892, 22782, 29892, 14707, 29892,   322,\n",
       "         1653,  2472, 29889,   739,   427,  2388,   465,   267,   263,  9377,\n",
       "         3464,   310, 25078, 29892,  3704,  6996,  6601, 25078, 29892,  8986,\n",
       "        11322, 29892,  7395, 12084, 29892,   322,   278, 11509,   304,  3994,\n",
       "         1711, 14707, 13436,  2793, 29889,   512,   263,  3186,   988,  2472,\n",
       "          338, 28520,  3625,   472,  1749,   285,   292,   814,  4512, 29892,\n",
       "          278, 11509,   304,  2313,   824,  6625,  1821,  8974,   515,   443,\n",
       "          276,   492,   519,  6743,   338,  7618,  1455, 29889,    13,    13,\n",
       "         1576, 13500,   310, 13436,  4631,  4135,   508,   367,  3595,   297,\n",
       "         5164, 21420,   310,  2834, 29889,   512,   278,   664,  6689, 29892,\n",
       "          363,  2777, 29892, 22873,   526,  3806,   304,   671, 13436,  8492,\n",
       "          304, 23120, 29892, 11465,   403, 29892,   322,  4866,  9595, 29497,\n",
       "        29889, 16025,  1058, 10225, 13436,  4631,  4135, 25078,  1122,  1284,\n",
       "         6053,   472,   263,   766, 17263,  8501, 29892, 20042,   304,  3013,\n",
       "          701,   411,  1009,   901, 13615,   635,  4048, 13308, 23056, 21628],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a0365",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 1000;\n",
    "    color:white;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    "img {\n",
    "    display: block;\n",
    "    margin: 0 auto;\n",
    "    width: 40em;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2>How does the Tokenizer Break Down Text?</h2>\n",
    "<p>There are three factors that dictate how a tokenizer breaks down an input prompt.</p>\n",
    "<ul>\n",
    "    <li>First, at model design time, the creator of the model chooses a tokenization method. Popular methods include <b>byte pair encoding(BPE)</b> (widely used by GPT models) and <b>WordPiece</b>(used by BERT). They aim to optimize an efficient set of tokens to represent a text dataset, but they arrive at it in different ways.</li>\n",
    "    <li>Second, after choosing the method, we need to make a number of tokenizer design choices like vocabulary size and what special tokens to use.</li>\n",
    "    <li>Third, the tokenizer needs to be trained on a specific dataset to establish the best vocabulary it can use to represent that dataset. Even if we set the same methods and parameters, a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset.</li>\n",
    "</ul>\n",
    "<h2>Word Versus Subword Versus Character Versus Byte Tokens</h2>\n",
    "<p>The tokenization scheme we just discussed is call <b>subword tokenization</b>. It's the most commonly used tokenization scheme but not the only one. The four notable ways to tokenize are shown below:</p>\n",
    "<img src=\"../image/four_ways_tokenize.jpg\">\n",
    "<p>\n",
    "    <ul>\n",
    "        <li><b>Word tokens</b>: This approach was common with earlier methods like <b>word2vec</b> but is being used less and less in NLP. Though of its usefulness, it led to be used outside of NLP for use cases such as recommendation systems. One challenge with this method is that the tokenizer may be unable to deal with new words that enter that dataset after the tokenizer was trained. This also results in a vocabulary that has a lot of tokens with minimal differences between them. (e.g., apology, apologize, apologetic, apologist). This latter challenge is resolved by <b>subword tokenization</b> as it has a token for <em>apolog</em>, and then suffix tokens (e.g., -y, -ize, -etic, -ist) that are common with many other tokens, resulting in a more expressive vocabulary.</li>\n",
    "        <li><b>Subword tokens</b>: This method contains full and partial words. In addition to the vocabulary expressivity mentioned earlier, another benefit of the approach is its ability to represent new words by breaking down the new token into smaller characters, which tend to be a part of the vocabulary.</li>\n",
    "        <li><b>Characters tokens</b>: It can deal successfully with new words because it has the raw letters to fall back on. While that makes the representation easier to tokenize, it makes the modeling more difficult. Where a model with subword tokenization can represent \"play\" as one token, a model using character-level tokens needs to model the information to spell out <b>\"p-l-a-y\"</b> in addition to modeling the rest of the sequence. Subword tokens present an advantage over character tokens in the ability to fit more text within the limited context length of a Transformer model.</li>\n",
    "        <li><b>Byte Tokens</b>: <a href=\"https://arxiv.org/abs/2103.06874\"><b style=\"color: red\">\"CANINE: Pre-training an efficient tokenization-free encoder for language representation\"</b> outline methods like this, which are also called <b>tokenization-free encoding</b>.</a> Other works like <a href=\"https://arxiv.org/abs/2105.13626\"><b style=\"color:red\">\"ByT5: Towards a token-free future with pre-trained byte-to-byte models.\"</b></a> show that this can be a competitive method, especially in multilingual scenarios.</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a5105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "show_tokens False None elif == >= else: two tabs:\" \" Three tabs:\n",
    "\" \"\n",
    "12.0*50=600\n",
    "\"\"\"\n",
    "\n",
    "colors_list = [\n",
    "    '102;194;165', '252;141;98', '141;160;203',\n",
    "    '231;138;195', '166;216;84', '255;217;47'\n",
    "    ]\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    \n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors_list[idx %\n",
    "            len(colors_list)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87ef90",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 1000;\n",
    "    color:white;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    "img {\n",
    "    display: block;\n",
    "    margin: 0 auto;\n",
    "    width: 40em;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<img src=\"../image/table_tokenize_1.jpg\" width=\"400\">  <img src=\"../image/table_tokenize_2.jpg\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab7e2b",
   "metadata": {},
   "source": [
    "`"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "learning_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
