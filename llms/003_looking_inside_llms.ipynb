{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5391e0de",
   "metadata": {},
   "source": [
    "# **CHAPTER 3: Looking Inside Large Language Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa6e445-c66e-4176-9c2b-d7d8ba795ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:16:26.345089Z",
     "iopub.status.busy": "2025-08-22T14:16:26.344869Z",
     "iopub.status.idle": "2025-08-22T14:18:04.301161Z",
     "shell.execute_reply": "2025-08-22T14:18:04.300422Z",
     "shell.execute_reply.started": "2025-08-22T14:16:26.345061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q \"transformers==4.43.3\" \"accelerate>=0.31.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e343146-79b4-4dc4-943c-dfdd6629468d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:18:04.303440Z",
     "iopub.status.busy": "2025-08-22T14:18:04.303201Z",
     "iopub.status.idle": "2025-08-22T14:18:09.467920Z",
     "shell.execute_reply": "2025-08-22T14:18:09.467084Z",
     "shell.execute_reply.started": "2025-08-22T14:18:04.303417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers = 4.43.3\n",
      "torch = 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import transformers, torch\n",
    "print(\"transformers =\", transformers.__version__)\n",
    "print(\"torch =\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93675a65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:18:09.469095Z",
     "iopub.status.busy": "2025-08-22T14:18:09.468762Z",
     "iopub.status.idle": "2025-08-22T14:19:34.353604Z",
     "shell.execute_reply": "2025-08-22T14:19:34.353000Z",
     "shell.execute_reply.started": "2025-08-22T14:18:09.469069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 14:18:16.409736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755872296.759716      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755872296.860427      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62e763c5f0140138271f30ba7dbd401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c667b3343ced4896b119473c6737867c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4a3bed2284ed2a0f3f0874e53672e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca1dd83416143a7b1953d32212c0215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8611ebb7834400aafb87560ff39095b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc67e81b5ff4d31af683784f3bd23cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5e9d6249bd450a86bbb33b7878d0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1011aa43457f43e5825877fa85223140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15bfc7508ee4f6784ea7617821cb849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a306a14e316e4729a4ebf07d83a56b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7dd25bbb804986b3f0c76be8b8b572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8467d449d3443a6b6484490b756a5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055986a785b44cfd8e847a3394444f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c4e3ef53f34b02bd077f418c472ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "name_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_model)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name_model,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90c644f-0828-46c0-9fe6-7c2d0a27cd5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:19:34.355051Z",
     "iopub.status.busy": "2025-08-22T14:19:34.354425Z",
     "iopub.status.idle": "2025-08-22T14:19:34.359151Z",
     "shell.execute_reply": "2025-08-22T14:19:34.358372Z",
     "shell.execute_reply.started": "2025-08-22T14:19:34.355020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=120,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46902bf6-69b2-44a7-a260-7287241dba93",
   "metadata": {},
   "source": [
    "# **An Overview of Transformer Models**\n",
    "Let's begin our exploration with a high-level overview of the model, and then we'll see how later work has improved upon the Transformers model since its introduction in 2017.\n",
    "\n",
    "### **The Inputs and Outputs of a Trained Transformer LLM**\n",
    "When Transformer LLMs are given a prompt, they won't generate the text in one operation; it actually generates **one token** at a time. The picture below shows four steps of token generation in response to the input prompt. Each token generation step is one forward pass through the model (that's machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph).\n",
    "\n",
    "> So that it's called **autogressive** models\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/four_steps_transfomers.jpg\" style=\"display:block; margin:0 auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f4e4dd-844a-4ff5-a5de-9b5dabf923f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:19:34.360875Z",
     "iopub.status.busy": "2025-08-22T14:19:34.360007Z",
     "iopub.status.idle": "2025-08-22T14:19:51.263622Z",
     "shell.execute_reply": "2025-08-22T14:19:51.262805Z",
     "shell.execute_reply.started": "2025-08-22T14:19:34.360842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \" Mention the steps you're taking to prevent it in the future.\\n\\nDear Sarah,\\n\\nI hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden. It was a tragic mishap that I never intended, and I am truly sorry for any distress it may have caused you.\\n\\nThe incident happened when I was attempting to help you with your gardening project. I had been researching various techniques to improve the health and growth of your plants, and I thought I\"}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1133b6-8b68-40bd-8b0f-2bba52ac27d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:19:51.264843Z",
     "iopub.status.busy": "2025-08-22T14:19:51.264548Z",
     "iopub.status.idle": "2025-08-22T14:19:51.269320Z",
     "shell.execute_reply": "2025-08-22T14:19:51.268628Z",
     "shell.execute_reply.started": "2025-08-22T14:19:51.264823Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mention the steps you're taking to prevent it in the future.\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden. It was a tragic mishap that I never intended, and I am truly sorry for any distress it may have caused you.\n",
      "\n",
      "The incident happened when I was attempting to help you with your gardening project. I had been researching various techniques to improve the health and growth of your plants, and I thought I\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517fe532-00af-4405-9876-c816bdb40667",
   "metadata": {},
   "source": [
    "### **The Components of the Forward Pass**\n",
    "Two key internal components are the **tokenizer** and **the language modelling head (LM head)**. You can see where these components lie in the system. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/inside_transformers.jpg\" style=\"display:block; margin: 0 auto;\">\n",
    "\n",
    "The tokenizer is followed by the **neural network**: a stack of **Transformer** blocks that do all of the processing. That stack is then followed by the **LM head**, which translates the output of the stack into **probability scores** for what the most likely next token is. \n",
    "\n",
    "The **LM Head** is a simple neural network layer itself. It is one of multiple possible **\"heads\"** to attach to a stack of Transformer blocks to build different kinds of systems. Other kinds of Transformers heads include sequence classification heads and token classification heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c343b482-868b-4fc9-ae58-952e175c38dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:19:51.271135Z",
     "iopub.status.busy": "2025-08-22T14:19:51.270908Z",
     "iopub.status.idle": "2025-08-22T14:19:51.731048Z",
     "shell.execute_reply": "2025-08-22T14:19:51.730282Z",
     "shell.execute_reply.started": "2025-08-22T14:19:51.271116Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00f126-7a4f-463a-906c-61aed9ea0db4",
   "metadata": {},
   "source": [
    "As we can see in this structure, the **Phi3Model** contains:\n",
    "- This shows us the various nested layers of the model. The majority of the model is labelled **model** and **lm_head**.\n",
    "- Inside the **Phi3Model** model, we can see the **embeddings matrix** - *embed_tokens* and its dimensions. It had **32.064 tokens** and each with a vector size of **3.072**\n",
    "- The next major component is the stack of **Transformer** decoder layers. It contains **32** blocks of type **Phi3DecoderLayer**.\n",
    "- Each of these **Transformer blocks** includes an **attention layer** and a **feedforward neural network** (a.k.a an **mlp** or **multilevel perceptron**).\n",
    "- Finally, we see the **lm_head** taking a vector of size **3.072** and outputting a vector equivalent to the number of tokens the model knows. The output is the probability score for each token that helps us select the output token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafada3-a68e-421f-85cd-f0fd49188371",
   "metadata": {},
   "source": [
    "### **Choosing a Single Token from the Probability**\n",
    "At the end of processing, the model outputs a probability score for each token in the vocabulary. Choosing a single token from the probability distribution is called ***the decoding strategy***. The easiest decoding strategy would be to always pick the token with the highest probability score. In practice, however, this doesn't tend to lead to the best outputs for most use cases. A better approach is to add some **randomness** and sometimes choose the second or third highest probability token. The idea here is to basically **sample** from the probability distribution based on the probability score, as the statisticians would say.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/prob_tokens.jpg\" style=\"display:block; margin:0 auto;\">\n",
    "\n",
    "For example, in this case, the token \"Dear\" has a **40%** probability of being the next token, then it has a 40% chance of being picked. So with this method, all the other tokens have a chance of being picked according to their score.\n",
    "Choosing the highest-scoring token every time is called **greedy decoding**. It's what happens if you set the **temperature** parameter to zero in an LLM.\n",
    "\n",
    "Looking more closely at the code that demonstrates this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095bd4e2-6dd5-4def-805d-9fc0bbe98c9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:19:51.732706Z",
     "iopub.status.busy": "2025-08-22T14:19:51.731874Z",
     "iopub.status.idle": "2025-08-22T14:19:52.066374Z",
     "shell.execute_reply": "2025-08-22T14:19:52.065618Z",
     "shell.execute_reply.started": "2025-08-22T14:19:51.732676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 32064])\n",
      "tensor([[[24.7500, 24.8750, 22.7500,  ..., 19.0000, 19.0000, 19.0000],\n",
      "         [34.2500, 35.2500, 35.2500,  ..., 30.1250, 30.1250, 30.1250],\n",
      "         [33.5000, 32.0000, 34.7500,  ..., 29.6250, 29.6250, 29.6250],\n",
      "         [34.2500, 30.0000, 31.3750,  ..., 27.0000, 27.0000, 27.0000],\n",
      "         [34.5000, 33.2500, 36.2500,  ..., 29.0000, 29.0000, 29.0000],\n",
      "         [28.7500, 27.2500, 27.2500,  ..., 19.3750, 19.3750, 19.3750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ho'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The biggest city in Vietnam is\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "model_output = model.model(input_ids)\n",
    "\n",
    "lm_head_output = model.lm_head(model_output[0])\n",
    "print(lm_head_output.shape)\n",
    "print(lm_head_output)\n",
    "\n",
    "token_id = lm_head_output[0, -1].argmax(-1)\n",
    "tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c185b82-9b16-4ada-a550-886815481b23",
   "metadata": {},
   "source": [
    "### **Speeding up Generation by Caching Keys and Values**\n",
    "\n",
    "If we give the model the ability to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), we no longer need to repeat the calculations of the previous streams. This time, the only needed calculation is for the last stream. This is an optimization technique called the **keys and values (KV) cache**, and it provides a significant speedup of the generation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe5233de-bf95-4f00-83bc-a36fad41ee33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T15:09:01.895342Z",
     "iopub.status.busy": "2025-08-22T15:09:01.894650Z",
     "iopub.status.idle": "2025-08-22T15:12:52.467780Z",
     "shell.execute_reply": "2025-08-22T15:12:52.466983Z",
     "shell.execute_reply.started": "2025-08-22T15:09:01.895320Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.9 s ± 1.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "#* NOT USING CACHE\n",
    "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548ebe24-ad8a-453d-8065-199d1aa83204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T15:13:17.110557Z",
     "iopub.status.busy": "2025-08-22T15:13:17.110296Z",
     "iopub.status.idle": "2025-08-22T15:13:48.990423Z",
     "shell.execute_reply": "2025-08-22T15:13:48.989763Z",
     "shell.execute_reply.started": "2025-08-22T15:13:17.110540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55 s ± 42.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "#* NOT USING CACHE\n",
    "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186167a8-9395-4293-a41d-5399ac4f45ce",
   "metadata": {},
   "source": [
    "### **Inside the Transformer Block**\n",
    "\n",
    "As the picture shows, **Transformer LLMs** are composed of a series of Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs). Each block processes its input, then passes the results of its processing to the next block.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/inside_transformer_block.jpg\" style=\"display:block; margin: 0 auto;\">\n",
    "\n",
    "The Transformer block is made up of two successive components:\n",
    "- **The attention layer** is mainly concerned with incorporating.\n",
    "- **The feedforward layer** houses the majority of the model's processing capacity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/transformer_blocks_components.jpg\" style=\"display:block; margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1420d-fee9-4ded-8f79-7d56023945a4",
   "metadata": {},
   "source": [
    "## **Attention is All you need**\n",
    "**Attention** is a mechanism that helps the model incorporate context as it's processing a specific token.\n",
    "> The dog chased the squirrel because **it**\n",
    "\n",
    "For the model to predict what comes after \"it\", it needs to know what \"it\" refers to. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/attention_image.jpg\" style=\"display:block; margin:0 auto\">\n",
    "\n",
    "The picture shows multiple **token positions** going into the attention layer; the final one is the one being currently processed (the pink arrow). The attention mechanism operates on the input vector at that position. It incorporates relevant information from the context into the vector it produces as the output for that position.\n",
    "\n",
    "Two main steps are involved in the attention mechanism:\n",
    "- A way to score how relevant each of the previous input tokens are to the current token being processed (in the pink arrow)\n",
    "- Using those scores, we combine the information from the various positions into a single output vector.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/attention_two_steps.jpg\" style=\"display:block; margin:0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa9d88-538a-4748-b03d-b523d04d6f0e",
   "metadata": {},
   "source": [
    "## **How attention is calculated**\n",
    "\n",
    "Observe the following as the starting position:\n",
    "\n",
    "- The **attention** layer (of a generative LLM) is processing attention for a single position.\n",
    "- The inputs to the layer are:\n",
    "    + The vector representation of the current position or token\n",
    "    + The vector representations of the previous tokens\n",
    "- The goal is to produce a new representation of the current position that incorporates relevant information from the previous tokens:\n",
    "    + For example, the sentence **\"Sarah fed the cat because it\"**, we want **\"it\"** to represent the cat - so attention bakes in **\"cat information\"** from the cat token\n",
    "- The training process produces three **projection matrices** that produce the components that interact in this calculation:\n",
    "    + A **query** projection matrix\n",
    "    + A **key** projection matrix\n",
    "    + A **value** projection matrix\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/attention_head_how_it_calculated.jpg\" style=\"display:block; margin:0 auto\">\n",
    "\n",
    "## **Self-attention: Relevance scoring**\n",
    "\n",
    "The attention mechanism here is only concerned with this one position and how information from other positions can be pulled in to inform this position.\n",
    "\n",
    "The relevance scoring step of attention is conducted by **multiplying the query vector** of the current position with **the keys matrix**. This produces a **score stating** how relevant each previous token is. Passing that by a **softmax** operation normalizes these scores so they sum up to 1. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/attention_result.jpg\" style=\"display:block; margin:0 auto\">\n",
    "\n",
    "## **Self-attention: Combining information**\n",
    "\n",
    "After we have the relevance scores, we multiply the **value vector** associated with each token by that token's score. Summing up those resulting vectors produces the output of this attention step.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/final_attention_result.jpg\" style=\"display:block; margin:0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ed8f7-d74c-46d0-b93a-7b38eac53ad2",
   "metadata": {},
   "source": [
    "# **The Transformer Block**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/detail_transformer_block.jpg\" style=\"display:block; margin:0 auto\">\n",
    "\n",
    "One of the differences we see in this version of the Transformer block is that **normalization** happens prior to attention and the feedforward layers. This has been reported to reduce the required training time. Another improvement in normalization here is using **RMSNorm**, which is simpler and more efficient than the **LayerNorm** used in the original Transformer. Lastly, instead of the original Transformer's **ReLU activation function**, newer variants like **SwiGLU** are now more common.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/2024_era_transformer_block.jpg\" style=\"display:block; margin:0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126810c-8874-482c-b197-f7c2d55b0c59",
   "metadata": {},
   "source": [
    "## **Positional Embeddings (RoPE)**\n",
    "\n",
    "**Positional Embeddings** have been a key component since the original Transformer. They enable the model to keep track of the order of **tokens/words** in a sequence/sentence, which is an **indispensable** source of information in language. From the many positional encoding schemes proposed in the past years, **rotary positional embeddings** (or **RoPE**\n",
    "introduced in <a href=\"https://arxiv.org/abs/2104.09864v4\" style=\"color:#8e0012\">**“RoFormer: Enhanced Transformer with rotary position\n",
    "embedding”**</a>) \n",
    "\n",
    "The original Transformer paper and some of the early variants had absolute positional embedding that, in essence, marked the first token as **position 1**, the second as **position 2** ... et. But some challenges arise from such methods when we scale up models, which requires us to find ways to improve their efficiency.\n",
    "\n",
    "For example, one challenge in efficiently training models with **large context** is that a lot of documents in the training set are much shorter than that context. It would be inefficient to allocate the entire, say, **4K context** to a short **10-word sentence**. So during model training, documents are packed together into each context in the training batch.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/packed_documents.jpg\" style=\"display:block; margin:0 auto\">\n",
    "\n",
    "**Positional embedding** methods have to adapt to this and other practical considerations. If **Document 50**, for example, stars at position 50, then we'd be **misinforming** the model if we tell it that the first token is number 50, and that would affect its performance **(because it would assume there's preivous context while in reality the earlier tokens belong to a different and unrelated document the model should ignore)**\n",
    "\n",
    "Instead of static, absolute embeddings that are added at the beginning of the forward pass, rotary embeddings are a method to encode positional information in a way that captures absolute and relative token position information.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/khang1108/learning_ai/refs/heads/main/image/rotary_embeddings_position.jpg\" style=\"display:block; margin:0 auto\">"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "learning_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
