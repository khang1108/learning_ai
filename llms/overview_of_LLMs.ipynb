{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f2abc3",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 1000;\n",
    "    color:white;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    ".name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div>\n",
    "    <h2 style=\"text-align:center;\">Introduction to Large Language Models</h2>\n",
    "    <p>In this notebook, I've learned and built a strong foundation knowledge of LLMs with Python via Google's Course. The objectives of this course are:</p>\n",
    "    <ol>\n",
    "        <li>Define language models and large language models.</li>\n",
    "        <li>Define key LLM concepts, including Transformers and self-attention.</li>\n",
    "        <li>Describe the costs and benefits of LLMs, along with common use cases.</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cd071",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    color:white;\n",
    "    font-weight: 1000;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    ".name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    ".code-snippet{\n",
    "  background:#2b2b2b; color:#E8EAE3;\n",
    "  font-family:\"Fira Code\", monospace;\n",
    "  padding:10px 14px; border-radius:6px;\n",
    "  line-height:1.5; white-space:pre-wrap;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2>What is a language model</h2>\n",
    "<p>\n",
    "    A <b class =\"name_list\">language model</b> is a machine learning model that aims to predict and generate <b>plausible</b> language. These model work by estimating the probability of a token or sequence of tokens occurring within a longer sequence of tokens. A token could be a word, a subword (a subset of a word), or even a single character. Consider the following sentence:\n",
    "    <pre class=\"code-snippet\">When I hear rain on my roof, I _____ in my kitchen.</pre>\n",
    "    A language model determines the probabilities of different tokens or sequence of tokens to replace that underscore. For example, the following probability table identifies some possible tokens and their probabilities:\n",
    "    <pre class=\"code-snippet\">\n",
    "cook soup 9.4% \n",
    "warm up a kettle 5.2% \n",
    "cower 3.6% \n",
    "nap 2.5%\n",
    "relax 2.2%\n",
    "...\n",
    "    </pre>\n",
    "</p>\n",
    "<h2>N-gram language models</h2>\n",
    "<p>\n",
    "<b class=\"name_list\">N-grams</b> are ordered sequences of words used to build language models, where N is the number of words in the sequence. Given the following phrase in a training document:\n",
    "</p>\n",
    "<pre class=\"code-snippet\">you are very nice.</pre>\n",
    "<p>The resulting 2-grams are as follows:</p>\n",
    "<ul>\n",
    "    <li>you are</li>\n",
    "    <li>are very</li>\n",
    "    <li>very nice</li>\n",
    "</ul>\n",
    "<p>\n",
    "Given two words as input, a language model based on 3-grams can predict the likelihood of the third word, For example, given the following two words:\n",
    "</p>\n",
    "<pre class=\"code-snippet\">orange is</pre>\n",
    "<p>A language model examines all the different 3-grams derived from its training corpus that start with <b>orange is</b> to determine the most likely third word. Hundreds of 3-grams could start with the two words <b>orange is</b>, but you can focus solely on the following two possibilities:</p>\n",
    "<pre class=\"code-snippet\">orange is ripe<br>orange is cheerful</pre>\n",
    "<h2>Context</h2>\n",
    "<p>\n",
    "In language model, <b class=\"name_list\">context</b> is helpful information before or after the target token. Context can help a language model determine whether \"orange\" refers to a citrus fruit or a color. Longer N-grams would certainly provide more context than shorter N-grams. However, as N grows, the relative occurrence of each instance decreases. When N becomes very large, the language model typically has only a single instance of each occurrence of N tokens, which isn't very helpful in predicting the target token.\n",
    "</p>\n",
    "<h2>Recurrent neural networks</h2>\n",
    "<p>\n",
    "<b class=\"name_list\">Recurrent neural networks</b> provide more context than N-grams. A recurrent neural network is a type of neural network that trains on a sequence of tokens. For example, a recurrent neural network can gradually learn (and learn to ignore) selected context from each word in a sentence, kind of like you would when listening to someone speak. Although recurrent neural networks learn more context than N-grams, the amount of useful context recurrent neural networks can intuit is still relatively limited. It evaluate information \"token by token\". In contrast, large language models - the topic of the next section  - can evaluate the whole context at once.\n",
    "</p>\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250523171309383561/recurrent_neural_network.webp\" style=\"display:block; margin:0 auto\">\n",
    "<h2>Transformers</h2>\n",
    "<p>\n",
    "A key development in language modeling was the introduction in 2017 of Transformers, an architecture designed around the idea of attention. This made it possible to process longer sequences by focusing on the most important part of the input, solving memory issues encountered in earlier models.\n",
    "</p>\n",
    "<img src=\"https://developers.google.com/static/machine-learning/crash-course/images/Iamagooddog.png\" style=\"display:block; margin:0 auto\">\n",
    "<p>\n",
    "Full Transformers consist of an <b>encode and decoder</b>. \n",
    "<ul>\n",
    "    <li>An <a href=\"https://developers.google.com/machine-learning/glossary#encoder\"><b class=\"name_list\">encoder</b></a> converts input text into an intermediate representation. An encoder is an enormous neural net.</li>\n",
    "    <li>A <a href=\"https://developers.google.com/machine-learning/glossary#decoder\"><b class=\"name_list\">decoder</b></a> converts that intermediate representation into useful text. A decoder is also an enormous neural net.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250605171525709916/_what_are_transformers_.webp\" style=\"margin: 0 auto; display:block\">\n",
    "<p>\n",
    "Transformers architecture uses <b>self-attention</b> to transform one whole sentence into a single sentence. This is useful where older models work step by step and it helps overcome the challenges seen in models like <b>RNNs</b> and <b>LSTMs</b>. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aceddc",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 800;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    ".name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    ".code-snippet{\n",
    "  background:#2b2b2b; color:#E8EAE3;\n",
    "  font-family:\"Fira Code\", monospace;\n",
    "  padding:10px 14px; border-radius:6px;\n",
    "  line-height:1.5; white-space:pre-wrap;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
