{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f2abc3",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 1000;\n",
    "    color:white;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div>\n",
    "    <h2 style=\"text-align:center;\">Introduction to Large Language Models</h2>\n",
    "    <p>In this notebook, I've learned and built a strong foundation knowledge of LLMs with Python via Google's Course. The objectives of this course are:</p>\n",
    "    <ol>\n",
    "        <li>Define language models and large language models.</li>\n",
    "        <li>Define key LLM concepts, including Transformers and self-attention.</li>\n",
    "        <li>Describe the costs and benefits of LLMs, along with common use cases.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "<h2>Resources</h2>\n",
    "<a href=\"https://youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&si=qP79Lo_-OH1mLOf0\"><b class=\"name_list\">Stanford CS336 Language Modeling from Scratch.</b></a> <br>\n",
    "<a href=\"https://developers.google.com/machine-learning/crash-course\"><b class=\"name_list\">Google Machine Learning Crash Course</b></a>\n",
    "<p>Besides the above resources, I've also learned about LLMs through <b>Hands-On Large Language Models</b> book.</p>\n",
    "<img src=\"https://m.media-amazon.com/images/I/41NsAw-nT0L._SY445_SX342_.jpg\" style=\"margin:0 auto; display:block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cd071",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    color:white;\n",
    "    font-weight: 1000;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    ".code-snippet{\n",
    "  background:#2b2b2b; color:#E8EAE3;\n",
    "  font-family:\"Fira Code\", monospace;\n",
    "  padding:10px 14px; border-radius:6px;\n",
    "  line-height:1.5; white-space:pre-wrap;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2>What is a language model</h2>\n",
    "<p>\n",
    "    A <b class =\"name_list\">language model</b> is a machine learning model that aims to predict and generate <b>plausible</b> language. These model work by estimating the probability of a token or sequence of tokens occurring within a longer sequence of tokens. A token could be a word, a subword (a subset of a word), or even a single character. Consider the following sentence:\n",
    "    <pre class=\"code-snippet\">When I hear rain on my roof, I _____ in my kitchen.</pre>\n",
    "    A language model determines the probabilities of different tokens or sequence of tokens to replace that underscore. For example, the following probability table identifies some possible tokens and their probabilities:\n",
    "    <pre class=\"code-snippet\">\n",
    "cook soup 9.4% \n",
    "warm up a kettle 5.2% \n",
    "cower 3.6% \n",
    "nap 2.5%\n",
    "relax 2.2%\n",
    "...\n",
    "    </pre>\n",
    "</p>\n",
    "<h2>N-gram language models</h2>\n",
    "<p>\n",
    "<b class=\"name_list\">N-grams</b> are ordered sequences of words used to build language models, where N is the number of words in the sequence. Given the following phrase in a training document:\n",
    "</p>\n",
    "<pre class=\"code-snippet\">you are very nice.</pre>\n",
    "<p>The resulting 2-grams are as follows:</p>\n",
    "<ul>\n",
    "    <li>you are</li>\n",
    "    <li>are very</li>\n",
    "    <li>very nice</li>\n",
    "</ul>\n",
    "<p>\n",
    "Given two words as input, a language model based on 3-grams can predict the likelihood of the third word, For example, given the following two words:\n",
    "</p>\n",
    "<pre class=\"code-snippet\">orange is</pre>\n",
    "<p>A language model examines all the different 3-grams derived from its training corpus that start with <b>orange is</b> to determine the most likely third word. Hundreds of 3-grams could start with the two words <b>orange is</b>, but you can focus solely on the following two possibilities:</p>\n",
    "<pre class=\"code-snippet\">orange is ripe<br>orange is cheerful</pre>\n",
    "<h2>Context</h2>\n",
    "<p>\n",
    "In language model, <b class=\"name_list\">context</b> is helpful information before or after the target token. Context can help a language model determine whether \"orange\" refers to a citrus fruit or a color. Longer N-grams would certainly provide more context than shorter N-grams. However, as N grows, the relative occurrence of each instance decreases. When N becomes very large, the language model typically has only a single instance of each occurrence of N tokens, which isn't very helpful in predicting the target token.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477192fc",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    color:white;\n",
    "    font-weight: 1000;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    ".code-snippet{\n",
    "  background:#2b2b2b; color:#E8EAE3;\n",
    "  font-family:\"Fira Code\", monospace;\n",
    "  padding:10px 14px; border-radius:6px;\n",
    "  line-height:1.5; white-space:pre-wrap;\n",
    "}\n",
    "</style>\n",
    "<h2>Recurrent neural networks</h2>\n",
    "<p>\n",
    "<b class=\"name_list\">Recurrent neural networks</b> provide more context than N-grams. A recurrent neural network is a type of neural network that trains on a sequence of tokens. For example, a recurrent neural network can gradually learn (and learn to ignore) selected context from each word in a sentence, kind of like you would when listening to someone speak. Although recurrent neural networks learn more context than N-grams, the amount of useful context recurrent neural networks can intuit is still relatively limited. It evaluate information \"token by token\". In contrast, large language models - the topic of the next section  - can evaluate the whole context at once.\n",
    "</p>\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250523171309383561/recurrent_neural_network.webp\" style=\"display:block; margin:0 auto\">\n",
    "<h2>Transformers</h2>\n",
    "<p>\n",
    "A key development in language modeling was the introduction in 2017 of Transformers, an architecture designed around the idea of attention. This made it possible to process longer sequences by focusing on the most important part of the input, solving memory issues encountered in earlier models.\n",
    "</p>\n",
    "<img src=\"https://developers.google.com/static/machine-learning/crash-course/images/Iamagooddog.png\" style=\"display:block; margin:0 auto\">\n",
    "<p>\n",
    "Full Transformers consist of an <b>encode and decoder</b>. \n",
    "<ul>\n",
    "    <li>An <a href=\"https://developers.google.com/machine-learning/glossary#encoder\"><b class=\"name_list\">encoder</b></a> converts input text into an intermediate representation. An encoder is an enormous neural net.</li>\n",
    "    <li>A <a href=\"https://developers.google.com/machine-learning/glossary#decoder\"><b class=\"name_list\">decoder</b></a> converts that intermediate representation into useful text. A decoder is also an enormous neural net.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250605171525709916/_what_are_transformers_.webp\" style=\"margin: 0 auto; display:block\">\n",
    "<p>\n",
    "Transformers architecture uses <b>self-attention</b> to transform one whole sentence into a single sentence. This is useful where older models work step by step and it helps overcome the challenges seen in models like <b>RNNs</b> and <b>LSTMs</b>. \n",
    "</p>\n",
    "<h2>Self-attention</h2>\n",
    "<img src=\"../image/context_embedding.jpg\" style=\"margin: 0 auto; display:block\">\n",
    "<p>\n",
    "<b>RNN Encoder-Decoder (Seq2Seq)</b> uses <b>word2vec embeddings</b> to translate the original sentence to another sentence. This context embeddings, however, makes it difficult to deal with longer sentences since it is merely a single embedding representing the entire input. In 2014, a solution called <b class=\"para\">attention</b> was introduced that highly improved upon the original architecture. <b>Attention</b> allows a model to focus on parts of the input sequence that are relevant to one another (\"attend\" to each other). \n",
    "</p>\n",
    "<img src=\"../image/attention_example.jpg\" style=\"margin: 0 auto; display:block\">\n",
    "<p>\n",
    "To enhance context, Transformers rely heavily on a concept called <a href=\"https://developers.google.com/machine-learning/glossary#self-attention\"><b class=\"name_list\">self-attention</b></a>.  Effectively, on behalf of each token of input, self-attention asks the following question:\n",
    "</p>\n",
    "<pre class=\"code-snippet\">How much does each other token of input affect the interpretation of this token?</pre>\n",
    "<p>\n",
    "To simplify matters, assume that each token is a word and the complete context is only a single sentence. Consider the following sentence:\n",
    "</p>\n",
    "<pre class=\"code-snippet\">The animal didn't cross the street because it was too tired.</pre>\n",
    "<p>The preceding sentence contains eleven words. Each of the eleven words is paying attention to the other ten., wondering how much each of those ten words matters to itself. For example, the pronoun <b class=\"name_list\">it</b> typically refers to a recent noun or noun phrase, but in the example sentence, which recent noun does <b class=\"name_list\">it</b> refer to - the animal or the street?</p>\n",
    "<img src=\"https://developers.google.com/static/machine-learning/crash-course/images/Theanimaldidntcrossthestreet.png\" style=\"display:block; margin: 0 auto\">\n",
    "<p>Some self-attention mechanisms are <b class=\"name_list\">bidirectional</b>, meaning that they calculate relevance scores for tokens preceding and following the word being attended to. Encoders use bidirectional self-attention, while decoders use unidirectional</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aceddc",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: 800;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    ".name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    ".code-snippet{\n",
    "  background:#2b2b2b; color:#E8EAE3;\n",
    "  font-family:\"Fira Code\", monospace;\n",
    "  padding:10px 14px; border-radius:6px;\n",
    "  line-height:1.5; white-space:pre-wrap;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace028f",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    color:white;\n",
    "    font-weight: 1000;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    ".code-snippet{\n",
    "  background:#2b2b2b; color:#E8EAE3;\n",
    "  font-family:\"Fira Code\", monospace;\n",
    "  padding:10px 14px; border-radius:6px;\n",
    "  line-height:1.5; white-space:pre-wrap;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2>Representation Models: Encoder-Only models</h2>\n",
    "<p>\n",
    "The original Transformer model is an encoder-decoder architecture that serves translation tasks well but cannot easily be used for other tasks, like text classification. \n",
    "<br><br>\n",
    "<b>Bidirectional encoder Representations from Transformer (BERT)</b> was introduced that could be leveraged for a wide variety of tasks and would serve as the foundation of Language AI for years to come. <br><br>\n",
    "<b>BERT</b> is an <b>encoder-only architecture</b> that focuses on representing language. This means that it only uses encoder and removes the decoder entirely.\n",
    "</p>\n",
    "<img src=\"../image/architecture_BERT.jpg\" style=\"margin: 0 auto; display:block\">\n",
    "<p>\n",
    "But training these encoder stacks is a difficult task that BERT approaches by adopting a technique called <b class=\"name_list\">masked language modelling</b>. Although it is difficult, it allows BERT to create more accurate (intermediate) representations of the input.\n",
    "</p>\n",
    "<img src=\"../image/training_bert.jpg\" style=\"margin: 0 auto; display:block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13a470",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    color: #D6DAC8;\n",
    "    font-family: \"Fira Code\";\n",
    "    font-size: 14px;\n",
    "    font-weight: 550;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    color:white;\n",
    "    font-weight: 1000;\n",
    "}\n",
    "\n",
    ".para{\n",
    "    color: #DA6C6C;\n",
    "    font-weight: 950;\n",
    "}\n",
    "\n",
    "b, .name_list {\n",
    "    font-weight: 950;\n",
    "}\n",
    ".code-snippet{\n",
    "  background:#2b2b2b; color:#E8EAE3;\n",
    "  font-family:\"Fira Code\", monospace;\n",
    "  padding:10px 14px; border-radius:6px;\n",
    "  line-height:1.5; white-space:pre-wrap;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2>Generative Models: Decoder-Only models</h2>\n",
    "<p>\n",
    "A decoder-only architecture was proposed in 2018 to target generative tasks, called a <b>Generative Pre-trained Transformer (GPT)</b> for its capabilities (it's now known as GPT-1 to distinguish it from later versions).\n",
    "</p>\n",
    "<p>GPT-1 was trained on a corpus of 7,000 blocks and Common Crawl, a large dataset of web pages. The resulting model consisted of <b>117 million parameters.</b> Each parameters ia numerical value that represents the model's understanding of language.</p>\n",
    "<img src=\"../image/architecture_gpt1.jpg\" style=\"margin: 0 auto; display:block\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
